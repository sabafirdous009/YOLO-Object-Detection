{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording started: output_2024-12-02_10-52-04.mp4\n",
      "\n",
      "0: 480x640 1 person, 1956.2ms\n",
      "Speed: 12.4ms preprocess, 1956.2ms inference, 62.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1511.5ms\n",
      "Speed: 179.8ms preprocess, 1511.5ms inference, 19.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1268.0ms\n",
      "Speed: 49.2ms preprocess, 1268.0ms inference, 12.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1246.8ms\n",
      "Speed: 23.8ms preprocess, 1246.8ms inference, 14.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1598.6ms\n",
      "Speed: 54.8ms preprocess, 1598.6ms inference, 18.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 1278.4ms\n",
      "Speed: 37.3ms preprocess, 1278.4ms inference, 20.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1241.9ms\n",
      "Speed: 28.0ms preprocess, 1241.9ms inference, 18.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1368.9ms\n",
      "Speed: 69.0ms preprocess, 1368.9ms inference, 10.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1170.6ms\n",
      "Speed: 28.9ms preprocess, 1170.6ms inference, 14.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1221.7ms\n",
      "Speed: 33.2ms preprocess, 1221.7ms inference, 13.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 1348.6ms\n",
      "Speed: 18.6ms preprocess, 1348.6ms inference, 18.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1129.8ms\n",
      "Speed: 20.7ms preprocess, 1129.8ms inference, 20.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 1227.3ms\n",
      "Speed: 33.8ms preprocess, 1227.3ms inference, 10.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 cell phone, 1359.2ms\n",
      "Speed: 31.2ms preprocess, 1359.2ms inference, 12.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 2 cell phones, 1156.7ms\n",
      "Speed: 36.7ms preprocess, 1156.7ms inference, 16.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 cell phone, 1269.8ms\n",
      "Speed: 73.1ms preprocess, 1269.8ms inference, 13.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1429.5ms\n",
      "Speed: 36.3ms preprocess, 1429.5ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1403.3ms\n",
      "Speed: 21.0ms preprocess, 1403.3ms inference, 12.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1327.8ms\n",
      "Speed: 35.0ms preprocess, 1327.8ms inference, 18.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1386.4ms\n",
      "Speed: 32.5ms preprocess, 1386.4ms inference, 18.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Recording saved as output_2024-12-02_10-52-04.mp4\n"
     ]
    }
   ],
   "source": [
    "# objection detection and recording and saving\n",
    "\n",
    "import cv2\n",
    "import datetime\n",
    "from ultralytics import YOLO  # For YOLOv8, install with `pip install ultralytics`\n",
    "\n",
    "# Load YOLO model\n",
    "model = YOLO(\"yolov8n.pt\")  # Replace with your model file path\n",
    "\n",
    "# Initialize webcam\n",
    "cap = cv2.VideoCapture(0)  # Change index if you have multiple cameras\n",
    "\n",
    "# Get video properties\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS)) or 30\n",
    "\n",
    "# Create VideoWriter with date-time-stamped filename\n",
    "date_time = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "output_file = f\"output_{date_time}.mp4\"\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Codec\n",
    "out = cv2.VideoWriter(output_file, fourcc, fps, (frame_width, frame_height))\n",
    "\n",
    "print(f\"Recording started: {output_file}\")\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Perform object detection\n",
    "        results = model.predict(frame, conf=0.5)  # Adjust confidence threshold\n",
    "        annotated_frame = results[0].plot()  # Annotated frame\n",
    "\n",
    "        # Show the frame with detections\n",
    "        cv2.imshow(\"YOLO Object Detection\", annotated_frame)\n",
    "\n",
    "        # Write the frame to the output file\n",
    "        out.write(annotated_frame)\n",
    "\n",
    "        # Press 'q' to quit\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "finally:\n",
    "    # Release resources\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    print(f\"Recording saved as {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cv2\n",
    "# import datetime\n",
    "# import supervision as sv\n",
    "# from ultralytics import YOLO\n",
    "# import os\n",
    "\n",
    "\n",
    "# # Define the path to the weights file\n",
    "# # Load the model\n",
    "# model = YOLO(\"F:\\python envrovement\\webmobi360\\yolo11n.pt\")\n",
    "\n",
    "# def process_webcam():\n",
    "#     cap = cv2.VideoCapture(0)  # Replace with 0 for the default webcam\n",
    "#     cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)  # Set width\n",
    "#     cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)  # Set height\n",
    "#     cap.set(cv2.CAP_PROP_FPS, 30)  # Set FPS\n",
    "\n",
    "\n",
    "#     if not cap.isOpened():\n",
    "#         print(\"Error: Could not open video file.\")\n",
    "#         return\n",
    "\n",
    "#     bounding_box_annotator = sv.BoundingBoxAnnotator()\n",
    "#     label_annotator = sv.LabelAnnotator()\n",
    "\n",
    "#     while True:\n",
    "#         ret, frame = cap.read()\n",
    "#         if not ret:\n",
    "#             break\n",
    "\n",
    "#         results = model(frame)[0]\n",
    "#         detections = sv.Detections.from_ultralytics(results)\n",
    "\n",
    "#         annotated_frame = bounding_box_annotator.annotate(scene=frame, detections=detections)\n",
    "#         annotated_frame = label_annotator.annotate(scene=annotated_frame, detections=detections)\n",
    "\n",
    "#         # Display the annotated frame\n",
    "#         cv2.imshow(\"Webcam\", annotated_frame)\n",
    "#         if cv2.waitKey(25) & 0xFF == ord(\"q\"):\n",
    "#             break\n",
    "\n",
    "#     cap.release()\n",
    "#     cv2.destroyAllWindows()\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     print(\"Starting webcam processing...\")\n",
    "#     process_webcam()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting VisionEye Object Tracking and Detection...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SupervisionWarnings: BoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 person, 5685.1ms\n",
      "Speed: 41.7ms preprocess, 5685.1ms inference, 21.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1942.6ms\n",
      "Speed: 231.2ms preprocess, 1942.6ms inference, 28.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1415.0ms\n",
      "Speed: 51.9ms preprocess, 1415.0ms inference, 20.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1360.3ms\n",
      "Speed: 39.3ms preprocess, 1360.3ms inference, 20.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1313.5ms\n",
      "Speed: 43.7ms preprocess, 1313.5ms inference, 18.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1575.9ms\n",
      "Speed: 32.9ms preprocess, 1575.9ms inference, 10.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1393.4ms\n",
      "Speed: 30.6ms preprocess, 1393.4ms inference, 12.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1363.0ms\n",
      "Speed: 21.3ms preprocess, 1363.0ms inference, 10.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1400.8ms\n",
      "Speed: 47.0ms preprocess, 1400.8ms inference, 14.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1316.8ms\n",
      "Speed: 31.1ms preprocess, 1316.8ms inference, 21.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 cell phone, 1540.3ms\n",
      "Speed: 32.9ms preprocess, 1540.3ms inference, 8.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 cell phone, 1291.0ms\n",
      "Speed: 32.1ms preprocess, 1291.0ms inference, 20.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 cell phone, 1361.9ms\n",
      "Speed: 44.4ms preprocess, 1361.9ms inference, 20.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 cell phone, 1475.4ms\n",
      "Speed: 50.5ms preprocess, 1475.4ms inference, 9.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 cell phone, 1299.5ms\n",
      "Speed: 54.0ms preprocess, 1299.5ms inference, 22.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1562.0ms\n",
      "Speed: 34.0ms preprocess, 1562.0ms inference, 18.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 2380.7ms\n",
      "Speed: 89.1ms preprocess, 2380.7ms inference, 30.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 11363.5ms\n",
      "Speed: 21.1ms preprocess, 11363.5ms inference, 41.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 4698.4ms\n",
      "Speed: 145.1ms preprocess, 4698.4ms inference, 33.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 5270.0ms\n",
      "Speed: 37.5ms preprocess, 5270.0ms inference, 75.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Video saved as: visioneye_2024-12-02_10-58-37.avi\n"
     ]
    }
   ],
   "source": [
    "# import cv2\n",
    "# import datetime\n",
    "# import os\n",
    "# from ultralytics import YOLO\n",
    "# import supervision as sv\n",
    "\n",
    "# # Load YOLO model\n",
    "# model = YOLO(\"yolo11n.pt\")\n",
    "\n",
    "# # Function to get the current date and time as a string\n",
    "# def get_datetime_str():\n",
    "#     return datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "# # Function to process webcam feed with live detection and tracking\n",
    "# def process_webcam_with_tracking():\n",
    "#     cap = cv2.VideoCapture(0)  # Replace with 0 for default webcam\n",
    "#     cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)\n",
    "#     cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)\n",
    "#     cap.set(cv2.CAP_PROP_FPS, 30)\n",
    "\n",
    "#     if not cap.isOpened():\n",
    "#         print(\"Error: Could not open webcam.\")\n",
    "#         return\n",
    "\n",
    "#     # Get frame dimensions and FPS for video writer\n",
    "#     w, h, fps = (\n",
    "#         int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)),\n",
    "#         int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)),\n",
    "#         int(cap.get(cv2.CAP_PROP_FPS)),\n",
    "#     )\n",
    "\n",
    "#     # Define video output path with date and time\n",
    "#     output_filename = f\"visioneye_{get_datetime_str()}.avi\"\n",
    "#     out = cv2.VideoWriter(output_filename, cv2.VideoWriter_fourcc(*\"MJPG\"), fps, (w, h))\n",
    "\n",
    "#     bounding_box_annotator = sv.BoundingBoxAnnotator()\n",
    "#     label_annotator = sv.LabelAnnotator()\n",
    "\n",
    "#     while True:\n",
    "#         ret, frame = cap.read()\n",
    "#         if not ret:\n",
    "#             print(\"Video frame is empty or webcam processing completed.\")\n",
    "#             break\n",
    "\n",
    "#         # Process frame using YOLO model\n",
    "#         results = model.track(frame, persist=True)\n",
    "#         detections = sv.Detections.from_ultralytics(results[0])\n",
    "\n",
    "#         # Annotate frame with bounding boxes and labels\n",
    "#         annotated_frame = bounding_box_annotator.annotate(scene=frame, detections=detections)\n",
    "#         annotated_frame = label_annotator.annotate(scene=annotated_frame, detections=detections)\n",
    "\n",
    "#         # Draw tracking information (IDs) and custom logic\n",
    "#         if results[0].boxes.id is not None:\n",
    "#             track_ids = results[0].boxes.id.int().cpu().tolist()\n",
    "#             boxes = results[0].boxes.xyxy.cpu()\n",
    "#             for box, track_id in zip(boxes, track_ids):\n",
    "#                 cv2.putText(\n",
    "#                     annotated_frame,\n",
    "#                     f\"ID: {track_id}\",\n",
    "#                     (int(box[0]), int(box[1]) - 10),\n",
    "#                     cv2.FONT_HERSHEY_SIMPLEX,\n",
    "#                     0.5,\n",
    "#                     (255, 0, 0),\n",
    "#                     2,\n",
    "#                 )\n",
    "\n",
    "#         # Save annotated frame to video file\n",
    "#         out.write(annotated_frame)\n",
    "\n",
    "#         # Display the annotated frame\n",
    "#         cv2.imshow(\"VisionEye View with Tracking\", annotated_frame)\n",
    "\n",
    "#         # Break the loop if 'q' is pressed\n",
    "#         if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "#             break\n",
    "\n",
    "#     # Release resources\n",
    "#     cap.release()\n",
    "#     out.release()\n",
    "#     cv2.destroyAllWindows()\n",
    "#     print(f\"Video saved as: {output_filename}\")q\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     print(\"Starting VisionEye Object Tracking and Detection...\")\n",
    "#     process_webcam_with_tracking()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording started: output_2024-12-02_11-46-36.mp4\n",
      "\n",
      "0: 480x640 1 person, 2069.0ms\n",
      "Speed: 38.0ms preprocess, 2069.0ms inference, 32.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 5018.0ms\n",
      "Speed: 52.0ms preprocess, 5018.0ms inference, 50.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1855.0ms\n",
      "Speed: 299.0ms preprocess, 1855.0ms inference, 10.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1464.0ms\n",
      "Speed: 34.0ms preprocess, 1464.0ms inference, 21.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1531.0ms\n",
      "Speed: 97.0ms preprocess, 1531.0ms inference, 15.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 2163.0ms\n",
      "Speed: 46.0ms preprocess, 2163.0ms inference, 15.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1371.0ms\n",
      "Speed: 100.0ms preprocess, 1371.0ms inference, 18.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1384.0ms\n",
      "Speed: 42.0ms preprocess, 1384.0ms inference, 13.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1685.0ms\n",
      "Speed: 120.0ms preprocess, 1685.0ms inference, 16.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1519.0ms\n",
      "Speed: 71.0ms preprocess, 1519.0ms inference, 15.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 2769.6ms\n",
      "Speed: 84.0ms preprocess, 2769.6ms inference, 15.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 9685.0ms\n",
      "Speed: 34.0ms preprocess, 9685.0ms inference, 34.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 3519.0ms\n",
      "Speed: 86.0ms preprocess, 3519.0ms inference, 13.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 2138.0ms\n",
      "Speed: 49.0ms preprocess, 2138.0ms inference, 13.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1293.0ms\n",
      "Speed: 21.0ms preprocess, 1293.0ms inference, 7.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1279.0ms\n",
      "Speed: 48.0ms preprocess, 1279.0ms inference, 16.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1910.0ms\n",
      "Speed: 23.0ms preprocess, 1910.0ms inference, 7.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1205.0ms\n",
      "Speed: 22.0ms preprocess, 1205.0ms inference, 15.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1379.0ms\n",
      "Speed: 17.0ms preprocess, 1379.0ms inference, 14.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1214.0ms\n",
      "Speed: 87.0ms preprocess, 1214.0ms inference, 14.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1660.0ms\n",
      "Speed: 28.0ms preprocess, 1660.0ms inference, 9.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 2369.0ms\n",
      "Speed: 65.0ms preprocess, 2369.0ms inference, 59.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 2283.1ms\n",
      "Speed: 25.0ms preprocess, 2283.1ms inference, 15.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 3612.0ms\n",
      "Speed: 51.0ms preprocess, 3612.0ms inference, 14.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1286.0ms\n",
      "Speed: 89.0ms preprocess, 1286.0ms inference, 14.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 2899.0ms\n",
      "Speed: 19.0ms preprocess, 2899.0ms inference, 65.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 7145.0ms\n",
      "Speed: 265.0ms preprocess, 7145.0ms inference, 253.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 3720.0ms\n",
      "Speed: 170.0ms preprocess, 3720.0ms inference, 20.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 2334.0ms\n",
      "Speed: 421.0ms preprocess, 2334.0ms inference, 15.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1721.0ms\n",
      "Speed: 45.0ms preprocess, 1721.0ms inference, 15.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1968.1ms\n",
      "Speed: 142.0ms preprocess, 1968.1ms inference, 14.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 cell phone, 1682.0ms\n",
      "Speed: 148.0ms preprocess, 1682.0ms inference, 20.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 cell phone, 1802.0ms\n",
      "Speed: 86.0ms preprocess, 1802.0ms inference, 15.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 cell phone, 1839.0ms\n",
      "Speed: 21.0ms preprocess, 1839.0ms inference, 10.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 cell phone, 1628.0ms\n",
      "Speed: 127.0ms preprocess, 1628.0ms inference, 18.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 cell phone, 2980.0ms\n",
      "Speed: 81.0ms preprocess, 2980.0ms inference, 14.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Recording saved as output_2024-12-02_11-46-36.mp4\n"
     ]
    }
   ],
   "source": [
    "# objection detection and recording and saving\n",
    "\n",
    "import cv2\n",
    "import datetime\n",
    "from ultralytics import YOLO  # For YOLOv8, install with `pip install ultralytics`\n",
    "from ultralytics.utils.plotting import Annotator, colors\n",
    "\n",
    "# Load YOLO model\n",
    "# model = YOLO(\"yolov8n.pt\")  # Replace with your model file path\n",
    "model = YOLO(\"yolo11n.pt\")\n",
    "\n",
    "\n",
    "# Initialize webcam\n",
    "cap = cv2.VideoCapture(0)  # Change index if you have multiple cameras\n",
    "\n",
    "# Get video properties\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS)) or 30\n",
    "\n",
    "# Create VideoWriter with date-time-stamped filename\n",
    "date_time = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "output_file = f\"output_{date_time}.mp4\"\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Codec\n",
    "out = cv2.VideoWriter(output_file, fourcc, fps, (frame_width, frame_height))\n",
    "\n",
    "center_point = (-10, frame_height)\n",
    "\n",
    "\n",
    "print(f\"Recording started: {output_file}\")\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"Video frame is empty or video processing has been successfully completed.\")\n",
    "            break\n",
    "\n",
    "        annotator = Annotator(frame, line_width=2) #  Initialize Annotator\n",
    "\n",
    "        # Perform object detection\n",
    "        results = model.predict(frame, conf=0.5)  # Adjust confidence threshold\n",
    "        annotated_frame = results[0].plot()  # Annotated frame\n",
    "\n",
    "        results = model.track(frame, persist=True)  # Perform Object Detection and Tracking\n",
    "        boxes = results[0].boxes.xyxy.cpu()\n",
    "\n",
    "        if results[0].boxes.id is not None:\n",
    "            track_ids = results[0].boxes.id.int().cpu().tolist()\n",
    "\n",
    "\n",
    "            for box, track_id in zip(boxes, track_ids):\n",
    "                annotator.box_label(box, label=str(track_id), color=colors(int(track_id)))\n",
    "                annotator.visioneye(box, center_point)\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "        # Show the frame with detections\n",
    "        cv2.imshow(\"YOLO Object Detection\", annotated_frame)\n",
    "\n",
    "        # Write the frame to the output file\n",
    "        out.write(annotated_frame)\n",
    "\n",
    "        # Press 'q' to quit\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "finally:\n",
    "    # Release resources\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    print(f\"Recording saved as {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1/1: 0... Success  (inf frames of shape 640x480 at 30.00 FPS)\n",
      "\n",
      "\n",
      "WARNING  inference results will accumulate in RAM unless `stream=True` is passed, causing potential out-of-memory\n",
      "errors for large sources or long-running streams and videos. See https://docs.ultralytics.com/modes/predict/ for help.\n",
      "\n",
      "Example:\n",
      "    results = model(source=..., stream=True)  # generator of Results objects\n",
      "    for r in results:\n",
      "        boxes = r.boxes  # Boxes object for bbox outputs\n",
      "        masks = r.masks  # Masks object for segment masks outputs\n",
      "        probs = r.probs  # Class probabilities for classification outputs\n",
      "\n",
      "0: 480x640 1 person, 2 bananas, 337.9ms\n",
      "0: 480x640 1 person, 2 bananas, 334.8ms\n",
      "0: 480x640 1 person, 2 bananas, 314.6ms\n",
      "0: 480x640 1 person, 2 bananas, 306.4ms\n",
      "0: 480x640 1 person, 2 bananas, 320.5ms\n",
      "0: 480x640 1 person, 2 bananas, 496.7ms\n",
      "0: 480x640 1 person, 2 bananas, 589.3ms\n",
      "0: 480x640 1 person, 2 bananas, 574.3ms\n",
      "0: 480x640 1 person, 2 bananas, 569.3ms\n",
      "0: 480x640 1 person, 2 bananas, 566.5ms\n",
      "0: 480x640 1 person, 2 bananas, 549.2ms\n",
      "0: 480x640 1 person, 2 bananas, 590.9ms\n",
      "0: 480x640 1 person, 2 bananas, 612.2ms\n",
      "0: 480x640 1 person, 2 bananas, 994.5ms\n",
      "0: 480x640 1 person, 2 bananas, 1177.5ms\n",
      "0: 480x640 1 person, 2 bananas, 1029.9ms\n",
      "0: 480x640 1 person, 598.4ms\n",
      "0: 480x640 1 person, 2 bananas, 880.8ms\n",
      "0: 480x640 1 person, 2 bananas, 1 toothbrush, 1119.2ms\n",
      "0: 480x640 1 person, 2 bananas, 1 toothbrush, 1065.4ms\n",
      "0: 480x640 1 person, 2 bananas, 509.7ms\n",
      "0: 480x640 1 person, 1 banana, 545.5ms\n",
      "0: 480x640 1 person, 2 bananas, 556.4ms\n",
      "0: 480x640 1 person, 2 bananas, 546.0ms\n",
      "0: 480x640 1 person, 2 bananas, 546.7ms\n",
      "0: 480x640 1 person, 2 bananas, 519.7ms\n",
      "0: 480x640 1 person, 2 bananas, 573.0ms\n",
      "0: 480x640 1 person, 2 bananas, 545.8ms\n",
      "0: 480x640 1 person, 2 bananas, 547.7ms\n",
      "0: 480x640 1 person, 2 bananas, 544.4ms\n",
      "0: 480x640 1 person, 2 bananas, 533.3ms\n",
      "0: 480x640 1 person, 2 bananas, 532.6ms\n",
      "0: 480x640 1 person, 1 banana, 519.3ms\n",
      "0: 480x640 1 person, 1 banana, 546.2ms\n",
      "0: 480x640 1 person, 1 banana, 570.8ms\n",
      "0: 480x640 1 person, 2 bananas, 541.4ms\n",
      "0: 480x640 1 person, 2 bananas, 539.7ms\n",
      "0: 480x640 1 person, 1 banana, 535.7ms\n",
      "0: 480x640 1 person, 1 banana, 540.8ms\n",
      "0: 480x640 1 person, 2 bananas, 534.7ms\n",
      "0: 480x640 1 person, 2 bananas, 537.5ms\n",
      "0: 480x640 1 person, 2 bananas, 549.2ms\n",
      "0: 480x640 1 person, 2 bananas, 552.8ms\n",
      "0: 480x640 1 person, 2 bananas, 562.7ms\n",
      "0: 480x640 1 person, 1 banana, 548.9ms\n",
      "0: 480x640 1 person, 2 bananas, 547.6ms\n",
      "0: 480x640 1 person, 2 bananas, 544.2ms\n",
      "0: 480x640 1 person, 2 bananas, 536.0ms\n",
      "0: 480x640 1 person, 2 bananas, 529.6ms\n",
      "0: 480x640 1 person, 2 bananas, 539.2ms\n",
      "0: 480x640 1 person, 2 bananas, 533.4ms\n",
      "0: 480x640 1 person, 2 bananas, 539.7ms\n",
      "0: 480x640 1 person, 2 bananas, 566.3ms\n",
      "0: 480x640 1 person, 2 bananas, 574.2ms\n",
      "0: 480x640 1 person, 2 bananas, 611.3ms\n",
      "0: 480x640 1 person, 2 bananas, 596.0ms\n",
      "0: 480x640 1 person, 1 banana, 567.3ms\n",
      "0: 480x640 1 person, 1 banana, 544.9ms\n",
      "0: 480x640 1 person, 1 banana, 533.7ms\n",
      "0: 480x640 1 person, 1 banana, 550.3ms\n",
      "0: 480x640 1 person, 1 banana, 537.8ms\n",
      "0: 480x640 1 person, 2 bananas, 577.0ms\n",
      "0: 480x640 1 person, 1 banana, 571.9ms\n",
      "0: 480x640 1 person, 2 bananas, 537.4ms\n",
      "0: 480x640 1 person, 2 bananas, 538.2ms\n",
      "0: 480x640 1 person, 2 bananas, 532.9ms\n",
      "0: 480x640 1 person, 2 bananas, 530.2ms\n",
      "0: 480x640 1 person, 2 bananas, 502.7ms\n",
      "0: 480x640 1 person, 2 bananas, 568.5ms\n",
      "0: 480x640 1 person, 2 bananas, 537.8ms\n",
      "0: 480x640 1 person, 2 bananas, 528.3ms\n",
      "0: 480x640 1 person, 2 bananas, 533.7ms\n",
      "0: 480x640 1 person, 2 bananas, 543.4ms\n",
      "0: 480x640 1 person, 2 bananas, 533.2ms\n",
      "0: 480x640 1 person, 2 bananas, 531.7ms\n",
      "0: 480x640 1 person, 2 bananas, 546.8ms\n",
      "0: 480x640 1 person, 543.3ms\n",
      "0: 480x640 1 person, 1 banana, 525.0ms\n",
      "0: 480x640 1 person, 1 banana, 528.7ms\n",
      "0: 480x640 1 person, 1 banana, 533.3ms\n",
      "0: 480x640 1 person, 1 banana, 525.1ms\n",
      "0: 480x640 1 person, 2 bananas, 535.7ms\n",
      "0: 480x640 1 person, 2 bananas, 514.8ms\n",
      "0: 480x640 1 person, 545.3ms\n",
      "0: 480x640 1 person, 1 banana, 542.1ms\n",
      "0: 480x640 1 person, 536.4ms\n",
      "0: 480x640 1 person, 541.6ms\n",
      "0: 480x640 1 person, 2 bananas, 543.9ms\n",
      "0: 480x640 1 person, 2 bananas, 542.7ms\n",
      "0: 480x640 1 person, 2 bananas, 503.0ms\n",
      "0: 480x640 1 person, 1 banana, 568.2ms\n",
      "0: 480x640 1 person, 1 bowl, 553.3ms\n",
      "0: 480x640 1 person, 1 chair, 561.5ms\n",
      "0: 480x640 1 person, 1 chair, 554.7ms\n",
      "0: 480x640 1 person, 535.3ms\n",
      "0: 480x640 1 person, 544.1ms\n",
      "0: 480x640 1 person, 518.7ms\n",
      "0: 480x640 1 person, 1 banana, 595.9ms\n",
      "0: 480x640 1 person, 1 banana, 632.0ms\n",
      "0: 480x640 1 person, 1 banana, 595.5ms\n",
      "0: 480x640 1 person, 1 banana, 586.2ms\n",
      "0: 480x640 1 person, 1 banana, 560.0ms\n",
      "0: 480x640 1 person, 1 banana, 540.3ms\n",
      "0: 480x640 1 person, 1 banana, 539.4ms\n",
      "0: 480x640 1 person, 1 banana, 535.1ms\n",
      "0: 480x640 1 person, 1 banana, 557.9ms\n",
      "0: 480x640 1 person, 1 banana, 548.3ms\n",
      "0: 480x640 1 person, 1 banana, 562.3ms\n",
      "0: 480x640 1 person, 1 banana, 520.3ms\n",
      "0: 480x640 1 person, 1 banana, 494.5ms\n",
      "0: 480x640 1 person, 1 banana, 520.0ms\n",
      "0: 480x640 1 person, 1 banana, 524.2ms\n",
      "0: 480x640 1 person, 532.8ms\n",
      "0: 480x640 1 person, 1 banana, 510.0ms\n",
      "0: 480x640 1 person, 1 banana, 515.6ms\n",
      "0: 480x640 1 person, 1 banana, 516.3ms\n",
      "0: 480x640 1 person, 1 banana, 510.6ms\n",
      "0: 480x640 1 person, 1 banana, 502.4ms\n",
      "0: 480x640 1 person, 1 banana, 514.4ms\n",
      "0: 480x640 1 person, 1 chair, 505.3ms\n",
      "0: 480x640 1 person, 1 banana, 517.3ms\n",
      "0: 480x640 1 person, 1 chair, 508.4ms\n",
      "0: 480x640 1 person, 1 chair, 512.7ms\n",
      "0: 480x640 1 person, 1 chair, 497.9ms\n",
      "0: 480x640 1 person, 504.3ms\n",
      "0: 480x640 1 person, 1 banana, 472.4ms\n",
      "0: 480x640 1 person, 1 banana, 497.9ms\n",
      "0: 480x640 1 person, 1 banana, 529.7ms\n",
      "0: 480x640 1 person, 1 banana, 514.2ms\n",
      "0: 480x640 1 person, 1 banana, 494.2ms\n",
      "0: 480x640 1 person, 1 banana, 488.5ms\n",
      "0: 480x640 1 person, 1 banana, 509.1ms\n",
      "0: 480x640 1 person, 1 banana, 503.6ms\n",
      "0: 480x640 1 person, 1 banana, 490.3ms\n",
      "0: 480x640 1 person, 1 banana, 519.2ms\n",
      "0: 480x640 1 person, 1 banana, 499.9ms\n",
      "0: 480x640 1 person, 511.2ms\n",
      "0: 480x640 1 person, 1 banana, 509.9ms\n",
      "0: 480x640 1 person, 1 banana, 484.9ms\n",
      "0: 480x640 1 person, 1 banana, 494.3ms\n",
      "0: 480x640 1 person, 2 bananas, 518.7ms\n",
      "0: 480x640 1 person, 2 bananas, 501.4ms\n",
      "0: 480x640 1 person, 2 bananas, 503.9ms\n",
      "0: 480x640 1 person, 2 bananas, 507.5ms\n",
      "0: 480x640 1 person, 1 bowl, 500.4ms\n",
      "0: 480x640 1 person, 517.7ms\n",
      "0: 480x640 1 person, 1 banana, 501.3ms\n",
      "0: 480x640 1 person, 577.1ms\n",
      "0: 480x640 1 person, 1 banana, 554.6ms\n",
      "0: 480x640 1 person, 1 banana, 561.4ms\n",
      "0: 480x640 1 person, 1 banana, 543.9ms\n",
      "0: 480x640 1 person, 2 bananas, 540.8ms\n",
      "0: 480x640 1 person, 2 bananas, 526.3ms\n",
      "0: 480x640 1 person, 534.2ms\n",
      "0: 480x640 1 person, 2 bananas, 543.7ms\n",
      "0: 480x640 1 person, 2 bananas, 519.5ms\n",
      "0: 480x640 1 person, 2 bananas, 1002.9ms\n",
      "0: 480x640 1 person, 2 bananas, 814.8ms\n",
      "0: 480x640 1 person, 2 bananas, 775.8ms\n",
      "0: 480x640 1 person, 2 bananas, 906.4ms\n",
      "0: 480x640 1 person, 2 bananas, 838.7ms\n",
      "0: 480x640 1 person, 2 bananas, 867.6ms\n",
      "0: 480x640 1 person, 2 bananas, 793.5ms\n",
      "0: 480x640 1 person, 1 banana, 817.8ms\n",
      "0: 480x640 1 person, 1 banana, 829.2ms\n",
      "0: 480x640 1 person, 1 banana, 981.7ms\n",
      "0: 480x640 1 person, 1 banana, 904.3ms\n",
      "0: 480x640 1 person, 1 banana, 1063.8ms\n",
      "0: 480x640 1 person, 1 banana, 959.6ms\n",
      "0: 480x640 1 person, 1 banana, 887.7ms\n",
      "0: 480x640 1 person, 1 banana, 954.6ms\n",
      "0: 480x640 1 person, 1 banana, 1045.5ms\n",
      "0: 480x640 1 person, 1 banana, 844.9ms\n",
      "0: 480x640 1 person, 1 banana, 877.3ms\n",
      "0: 480x640 1 person, 1 banana, 814.5ms\n",
      "0: 480x640 1 person, 863.0ms\n",
      "0: 480x640 1 person, 1 banana, 896.7ms\n",
      "0: 480x640 1 person, 1 banana, 939.1ms\n",
      "0: 480x640 1 person, 1 banana, 1056.0ms\n",
      "0: 480x640 1 person, 1 banana, 1067.9ms\n",
      "0: 480x640 1 person, 1 banana, 911.1ms\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Load a COCO-pretrained YOLOv8n model\n",
    "model = YOLO(\"yolov8m.pt\")\n",
    "\n",
    "# Display model information (optional)\n",
    "# model.info()\n",
    "\n",
    "# Train the model on the COCO8 example dataset for 100 epochs\n",
    "results = model.track(tracker=\"botsort.yaml\", show=True, source=0)\n",
    "\n",
    "# Run inference with the YOLOv8n model on the 'bus.jpg' image\n",
    "# print(results)\n",
    "\n",
    "#botsort.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import cv2\n",
    "import datetime\n",
    "import os\n",
    "import mysql.connector\n",
    "from ultralytics import YOLO  # For YOLO, install with `pip install ultralytics`\n",
    "from ultralytics.utils.plotting import Annotator\n",
    "\n",
    "# Load YOLO model\n",
    "model = YOLO(\"yolo11n.pt\")  # Replace with your model file path\n",
    "\n",
    "# Initialize webcam\n",
    "cap = cv2.VideoCapture(0)  # Change index if you have multiple cameras\n",
    "\n",
    "# Get video properties\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS)) or 30\n",
    "\n",
    "# Create VideoWriter with date-time-stamped filename\n",
    "date_time = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "output_file = f\"output_{date_time}.mp4\"\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Codec\n",
    "out = cv2.VideoWriter(output_file, fourcc, fps, (frame_width, frame_height))\n",
    "\n",
    "# Directory to save detected images\n",
    "save_dir = \"detected_images\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# Connect to MySQL database\n",
    "db = mysql.connector.connect(\n",
    "    host=\"localhost\",\n",
    "    user=\"Your_User\",       # Replace with your MySQL username\n",
    "    password=\"your_password\",   # Replace with your MySQL password\n",
    "    database=\"yolo_detection\",  # Ensure the database exists\n",
    "    port=\"3306\"\n",
    ")\n",
    "cursor = db.cursor()\n",
    "\n",
    "print(f\"Recording started: {output_file}\")\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"Video frame is empty or video processing has been successfully completed.\")\n",
    "            break\n",
    "\n",
    "        # Perform object detection\n",
    "        results = model.predict(frame, conf=0.5)  # Adjust confidence threshold\n",
    "        boxes = results[0].boxes  # Get bounding boxes\n",
    "\n",
    "        # Only proceed if objects are detected\n",
    "        if len(boxes) > 0:\n",
    "            # Annotate detected objects on the frame\n",
    "            annotated_frame = results[0].plot()  # Annotated frame\n",
    "\n",
    "            \n",
    "            for box in boxes:\n",
    "                class_name = model.names[int(box.cls)]  # Get class name\n",
    "                confidence = float(box.conf)  # Confidence score\n",
    "\n",
    "                # Save the frame as an image with a timestamp\n",
    "                timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")  # Corrected timestamp format\n",
    "                image_filename = f\"{save_dir}/detected_{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S-%f')}.jpg\"\n",
    "                cv2.imwrite(image_filename, frame)  # Save the frame as an image\n",
    "                print(f\"Image saved: {image_filename}\")\n",
    "\n",
    "                # Insert detection data into the database\n",
    "                insert_query = \"\"\"\n",
    "                INSERT INTO detections (timestamp, detected_class, confidence, image_path)\n",
    "                VALUES (%s, %s, %s, %s)\n",
    "                \"\"\"\n",
    "                cursor.execute(insert_query, (timestamp, class_name, confidence, image_filename))\n",
    "                db.commit()\n",
    "                print(f\"Detection saved to database: {class_name} ({confidence:.2f})\")\n",
    "\n",
    "\n",
    "            # Write the annotated frame to the video output\n",
    "            out.write(annotated_frame)\n",
    "\n",
    "            # Display the annotated frame\n",
    "            cv2.imshow(\"YOLO Object Detection\", annotated_frame)\n",
    "        else:\n",
    "            # If no objects are detected, display the original frame\n",
    "            cv2.imshow(\"YOLO Object Detection\", frame)\n",
    "\n",
    "        # Press 'q' to quit\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "finally:\n",
    "    # Release resources\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    cursor.close()\n",
    "    db.close()\n",
    "    print(f\"Recording saved as {output_file}\")\n",
    "\n",
    "\n",
    "\n",
    "# CREATE DATABASE yolo_detection;\n",
    "\n",
    "# USE yolo_detection;\n",
    "\n",
    "# CREATE TABLE detections (\n",
    "#     id INT AUTO_INCREMENT PRIMARY KEY,\n",
    "#     timestamp DATETIME NOT NULL,\n",
    "#     detected_class VARCHAR(255) NOT NULL,\n",
    "#     confidence FLOAT NOT NULL,\n",
    "#     image_path VARCHAR(255) NOT NULL\n",
    "# );\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ODvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
